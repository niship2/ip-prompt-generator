import type { ReferenceMaterial } from './types';

export const REFERENCE_MATERIALS: ReferenceMaterial[] = [
  { title: 'AI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¨­è¨ˆã¨å®Ÿè£…', url: 'https://zenn.dev/aishift/articles/6aa1540ea27fcd' },
  { title: 'AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãŸã‚ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼šManusæ§‹ç¯‰ã‹ã‚‰å¾—ãŸæ•™è¨“', url: 'https://manus.im/ja/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus' },
  { title: 'Claude 4 ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹', url: 'https://docs.anthropic.com/ja/docs/build-with-claude/prompt-engineering/claude-4-best-practices' },
  { title: 'Context Engineering (LangChain Blog)', url: 'https://www.promptingguide.ai/guides/context-engineering-guide' },
  { title: 'Context Rot: How Increasing Input Tokens Impacts LLM Performance | Chroma Research', url: 'https://research.trychroma.com/context-rot' },
  { title: 'How to Fix Your Context', url: 'https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html' },
  { title: 'LLM ã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹é€ åŒ–ã•ã‚ŒãŸæ–‡æ›¸ã§ç®¡ç†ã™ã‚‹ POML', url: 'https://azukiazusa.dev/blog/poml-prompt-structured-document/' },
  { title: 'Prompt Engineering Guide', url: 'https://www.promptingguide.ai/' },
  { title: 'Prompt Engineering åŸºæœ¬ã‹ã‚‰å¿œç”¨ã¾ã§ã€ å®Ÿè·µãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚¯ãƒ‹ãƒƒã‚¯', url: 'https://services.google.com/fh/files/misc/promptengineeringwhitepaper.pdf' },
  { title: 'Prompt-Engineering-Guide: ğŸ™ Guides, papers, lecture, notebooks and resources for prompt engineering', url: 'https://github.com/dair-ai/Prompt-Engineering-Guide' },
  { title: 'The Product Compass A Guide to Context Engineering for PMs', url: 'https://www.productcompass.pm/p/context-engineering' },
  { title: 'ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°(Context Engineering) ã£ã¦ãªã‚“ã§æµè¡Œã£ã¦ã‚‹ã®ï¼Ÿ', url: 'https://tech.akariinc.co.jp/entry/2025/08/15/190000' }
];
